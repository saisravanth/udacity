{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, substring\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import isnan, when, count, avg\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dl.cfg']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('aws', 'AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('aws', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\" Create spark session \"\"\"\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def drop_rows_and_removeDuplicates(df, dropColumns):\n",
    "    print(\"drop rows with nulls in all columns of data frame: \", dropColumns)\n",
    "    print(\"Rows before dropping nulls: \", df.count())\n",
    "    df = df.na.drop(subset=dropColumns)                                       # In order to remove Rows with NULL values on all columns of PySpark DataFrame\n",
    "    print(\"Rows after dropping nulls: \", df.count())\n",
    "    df = df.distinct()\n",
    "    print(\"Rows after dropping duplicate rows: \", df.count())\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These values are obtained by running other notebook named 'Capstone Project Template.ipynb', esentially getting columns that have null values exceeding 5%.\n",
    "\n",
    "dropList_immigration = ['visapost', 'occup', 'entdepu', 'gender', 'insnum']\n",
    "dropList_airportCodes =  ['elevation_ft', 'continent', 'municipality', 'gps_code', 'iata_code', 'local_code']\n",
    "dropList_stateTemperature = []\n",
    "dropList_demographics = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_US_demographics_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Fetch us-cities-demographics data from S3, processes it and extract demographics from it.\n",
    "    Convert the data frames to parquet file and loaded back to S3 as output_data.\n",
    "        \n",
    "    Parameters:\n",
    "        spark       : Spark Session\n",
    "        input_data  : CSV file, location in S3 bucket\n",
    "        output_data : Parquet format stored in S3\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get filepath to us-cities-demographics data file\n",
    "    data = input_data + 'us-cities-demographics.csv'\n",
    "\n",
    "    # read data file\n",
    "    df = spark.read.csv(data, inferSchema=True, header=True,  sep=';')\n",
    "    \n",
    "    # remove specific columns as per notebook 'Capstone Project Template.ipynb':\n",
    "    df = df.drop(*dropList_demographics)\n",
    "    \n",
    "    # perform data cleaning\n",
    "    df = drop_rows_and_removeDuplicates(df, df.columns)\n",
    "    \n",
    "    df = df.withColumnRenamed('State Code', 'State_Code') \\\n",
    "        .withColumnRenamed('Male Population', 'Male_Population') \\\n",
    "        .withColumnRenamed('Female Population', 'Female_Population') \\\n",
    "        .withColumnRenamed('Total Population', 'Total_Population') \\\n",
    "        .withColumnRenamed('Number of Veterans', 'Number_Of_Veterans') \\\n",
    "        .withColumnRenamed('Average Household Size', 'Average_Household_Size') \\\n",
    "        .withColumnRenamed('Foreign-born', 'Foreign_Born') \\\n",
    "        .withColumnRenamed('Median Age', 'Median_Age') \\\n",
    "    \n",
    "    # modify data frame to meet requirements\n",
    "    df = df.groupBy(\"State_Code\", \"State\") \\\n",
    "        .agg(\n",
    "             _sum(\"Male_Population\").alias(\"Male_Population\"), \\\n",
    "             _sum(\"Female_Population\").alias(\"Female_Population\"), \\\n",
    "             _sum(\"Total_Population\").alias(\"Total_Population\"), \\\n",
    "             _sum(\"Number_Of_Veterans\").alias(\"Number_Of_Veterans\"), \\\n",
    "             _sum(\"Foreign_Born\").alias(\"Foreign_Born\"), \\\n",
    "             avg(\"Average_Household_Size\").alias(\"Average_Household_Size\"), \\\n",
    "             func.mean(df[\"Median_Age\"]).alias(\"Median_Age\") \\\n",
    "         ) \n",
    "    \n",
    "    # Adding an id column\n",
    "    df = df.withColumn(\"Id\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "    # write dimension to parquet file\n",
    "    df.write.parquet(output_data + \"Demographics\", mode=\"overwrite\")\n",
    "\n",
    "    df.show()\n",
    "    \n",
    "    df.printSchema()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_airport_codes_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Fetch airport_codes data from S3, processes it and extract airport details from it.\n",
    "    Convert the data frames to parquet files and loaded back to S3 as output_data.\n",
    "        \n",
    "    Parameters:\n",
    "        spark       : Spark Session\n",
    "        input_data  : CSV file, location in S3 bucket\n",
    "        output_data : Parquet format stored in S3\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get filepath to airport_codes data file\n",
    "    data = input_data + 'airport-codes_csv.csv'\n",
    "    \n",
    "    # read airport_codes data file\n",
    "    df = spark.read.csv(data, inferSchema=True, header=True,  sep=',')  \n",
    "    \n",
    "    # remove specific columns as per notebook 'Capstone Project Template.ipynb':\n",
    "    df = df.drop(*dropList_airportCodes)\n",
    "\n",
    "    # extract columns to create airport_codes dimension table \n",
    "    df = drop_rows_and_removeDuplicates(df, df.columns)\n",
    "    \n",
    "    # add a new state_code column:\n",
    "    df = df.select(df.ident, df.type, df.name, df.iso_country, df.iso_region, substring('iso_region', 4, 2).alias('state_code'), df.coordinates)\n",
    "    \n",
    "    # Adding an id column\n",
    "    df = df.withColumn('Id', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "    # write dimension to parquet file\n",
    "    df.write.parquet(output_data + \"Airport_Details\", mode=\"overwrite\")\n",
    "    \n",
    "    df.show()\n",
    "    \n",
    "    df.printSchema()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_codes = {'alabama': 'AL', 'alaska': 'AK', 'arizona': 'AZ', 'arkansas': 'AR', 'california': 'CA', 'colorado': 'CO', 'connecticut': 'CT', 'delaware': 'DE', 'dist. of columbia': 'DC', 'florida': 'FL', 'georgia': 'GA', 'guam': 'GU', 'hawaii': 'HI', 'idaho': 'ID', 'illinois': 'IL', 'indiana': 'IN', 'iowa': 'IA', 'kansas': 'KS', 'kentucky': 'KY', 'louisiana': 'LA', 'maine': 'ME', 'maryland': 'MD', 'massachusetts': 'MA', 'michigan': 'MI', 'minnesota': 'MN', 'mississippi': 'MS', 'missouri': 'MO', 'montana': 'MT', 'n. carolina': 'NC', 'n. dakota': 'ND', 'nebraska': 'NE', 'nevada': 'NV', 'new hampshire': 'NH', 'new jersey': 'NJ', 'new mexico': 'NM', 'new york': 'NY', 'ohio': 'OH', 'oklahoma': 'OK', 'oregon': 'OR', 'pennsylvania': 'PA', 'puerto rico': 'PR', 'rhode island': 'RI', 's. carolina': 'SC', 's. dakota': 'SD', 'tennessee': 'TN', 'texas': 'TX', 'utah': 'UT', 'vermont': 'VT', 'virgin islands': 'VI', 'virginia': 'VA', 'w. virginia': 'WV', 'washington': 'WA', 'wisconson': 'WI', 'wyoming': 'WY', 'all other codes': '99'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convertStateToStateCodes(value):\n",
    "    value = str.lower(value)\n",
    "    if value in state_codes.keys(): \n",
    "      return state_codes[value]\n",
    "    else:\n",
    "      return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_GlobalLandTemperaturesByState_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Fetch GlobalLandTemperaturesByState data from S3, processes it and extract GlobalLandTemperatures from it.\n",
    "    Convert the data frames to parquet files and loaded back to S3 as output_data.\n",
    "        \n",
    "    Parameters:\n",
    "        spark       : Spark Session\n",
    "        input_data  : Input json files location in S3 bucket\n",
    "        output_data : Parquet format stored in S3\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get filepath to data file\n",
    "    data = input_data + 'GlobalLandTemperaturesByState.csv'\n",
    "    \n",
    "    # read data file\n",
    "    df = spark.read.csv(data, inferSchema=True, header=True,  sep=',')  \n",
    "    \n",
    "    # remove specific columns as per notebook 'Capstone Project Template.ipynb':\n",
    "    df = df.drop(*dropList_stateTemperature)\n",
    "    \n",
    "    # filtering results for United States\n",
    "    df = df.filter(\"Country == 'United States'\")\n",
    "    \n",
    "    # extract columns to create GlobalLandTemperaturesByCity dimension table  \n",
    "    df = drop_rows_and_removeDuplicates(df, df.columns)\n",
    "    \n",
    "    # rounding temperature columns\n",
    "    df = df.withColumn(\"AverageTemperature\", func.round(df[\"AverageTemperature\"], 2))\n",
    "    df = df.withColumn(\"AverageTemperatureUncertainty\", func.round(df[\"AverageTemperatureUncertainty\"], 2))\n",
    "    \n",
    "    # add state_codes column to the data frame\n",
    "    udf_Function = func.udf(convertStateToStateCodes, StringType())\n",
    "    df = df.withColumn(\"State_Codes\", udf_Function(\"State\"))\n",
    "\n",
    "    # Adding an id column\n",
    "    df = df.withColumn('Id', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    df.write.parquet(output_data + \"Temperature_Details\", mode=\"overwrite\")\n",
    "    df.show()\n",
    "    df.printSchema()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extractColumnsForDateTime(df):\n",
    "    \"\"\"\n",
    "    Fetch date and time details from timestamp data. \n",
    "        \n",
    "    Parameters:\n",
    "        input_data  : Spark Data Frame\n",
    "        output_data : Spark Data Frame\n",
    "\n",
    "    \"\"\"\n",
    "    df = df.select(col('Id'), \n",
    "                        col('datetime').alias('Start_Time'), \n",
    "                        hour(col('datetime')).alias('Hour'),\n",
    "                        dayofmonth(col('datetime')).alias('Day'),\n",
    "                        weekofyear(col('datetime')).alias('Week'),\n",
    "                        month(col('datetime')).alias('Month'),\n",
    "                        year(col('datetime')).alias('Year'),\n",
    "                        date_format(col('datetime'), \"u\").alias('Weekday')\n",
    "                        ).dropDuplicates()\n",
    "    \n",
    "    df.printSchema()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createArrivalDateTimeDimensionTable(df, output_data):\n",
    "    \"\"\"\n",
    "    Used to create ArrivalDateTime dimension table.\n",
    "    \n",
    "    Parameters:\n",
    "        df: spark dataframe of Immigration data\n",
    "        output_data: path to write dimension dataframe to\n",
    "        return: spark dataframe of DateTime dimension\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: str(datetime.fromtimestamp(int(x) / 1000)))\n",
    "    df = df.withColumn(\"datetime\", get_datetime(df.arrdate))\n",
    "    \n",
    "    # extract columns\n",
    "    df = extractColumnsForDateTime(df)\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    df.write.partitionBy(\"year\", \"month\").mode('overwrite').parquet(output_data + \"ArrivalDateTime.parquet\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createDepartureDateTimeDimensionTable(df, output_data):\n",
    "    \"\"\"\n",
    "    Used to create DepartureDateTime dimension table.\n",
    "    \n",
    "    Parameters:\n",
    "        df: spark dataframe of Immigration data\n",
    "        output_data: path to write dimension dataframe to\n",
    "        return: spark dataframe of DateTime dimension\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: str(datetime.fromtimestamp(int(x) / 1000)))\n",
    "    df = df.withColumn(\"datetime\", get_datetime(df.depdate))\n",
    "    \n",
    "    # extract columns\n",
    "    df = extractColumnsForDateTime(df)\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    df.write.partitionBy(\"year\", \"month\").mode('overwrite').parquet(output_data + \"DepartureDateTime.parquet\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_immigration_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Fetch Immigration data from S3 and processes it.\n",
    "    Convert the data frames to parquet files and loaded back to S3 as output_data.\n",
    "        \n",
    "    Parameters:\n",
    "        spark       : Spark Session\n",
    "        input_data  : Input parquet files location in S3 bucket\n",
    "        output_data : Parquet format stored in S3\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get filepath to immigration data file\n",
    "    data = input_data + 'sas_data/' + '*.parquet'\n",
    "    \n",
    "    # read immigration data file\n",
    "    df = spark.read.parquet(data) \n",
    "    \n",
    "    # remove specific columns as per notebook 'Capstone Project Template.ipynb':\n",
    "    df = df.drop(*dropList_immigration)\n",
    "    \n",
    "    # extract columns to create immigration table    \n",
    "    df = drop_rows_and_removeDuplicates(df, df.columns)\n",
    "    \n",
    "    # Adding an id column\n",
    "    df = df.withColumn(\"Id\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    \n",
    "    # create ArrivalDateTime dimension table\n",
    "    arrival_df = createArrivalDateTimeDimensionTable(df, output_data)\n",
    "    \n",
    "    # create DepartureDateTime dimension table\n",
    "    departure_df = createDepartureDateTimeDimensionTable(df, output_data)\n",
    "    \n",
    "    print(\"ArrivalDateTimeDimensionTable\")\n",
    "    arrival_df.show()\n",
    "    \n",
    "    print(\"DepartureDateTimeDimensionTable\")\n",
    "    departure_df.show()\n",
    "    \n",
    "    dropColumns = ['depdate', 'arrdate']\n",
    "    df = df.drop(*dropColumns)\n",
    "    \n",
    "    # write Fact Table to parquet file\n",
    "    df.write.parquet(output_data + \"Immigration_Details\", mode=\"overwrite\")\n",
    "    df.show()\n",
    "    df.printSchema()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def performDataQualityChecks(df):\n",
    "    \"\"\"\n",
    "    Perform quality checks on data.\n",
    "        \n",
    "    Parameters:\n",
    "        df       : Spark data frame \n",
    "\n",
    "    \"\"\"\n",
    "    if df.count() < 1:\n",
    "        raise ValueError(\"Number of rows in data frame is 0 !!\")\n",
    "    if len(df.columns) < 1:\n",
    "        raise ValueError(\"Number of columns in data frame is 0 !!\")\n",
    "    if 'pyspark' not in str(type(df)):\n",
    "        raise TypeError(\"Unexpected input type\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    \"\"\"\n",
    "    Main method to clean input data and create Fact and Dimension parquet tables.\n",
    "    \"\"\"\n",
    "\n",
    "    spark = create_spark_session()\n",
    "    #input_data = \"s3://aws-logs-464689066216-us-west-2/data/\"\n",
    "    #output_data = \"s3://aws-logs-464689066216-us-west-2/data/\"\n",
    "    \n",
    "    input_data = \"/home/workspace/data/\"\n",
    "    output_data = \"/home/workspace/data/\"\n",
    "    \n",
    "    df = create_US_demographics_data(spark, input_data, output_data)\n",
    "    performDataQualityChecks(df)\n",
    "    print(\"\\n\\n\")\n",
    "    df = process_airport_codes_data(spark, input_data, output_data)\n",
    "    performDataQualityChecks(df)\n",
    "    print(\"\\n\\n\")\n",
    "    df = process_GlobalLandTemperaturesByState_data(spark, input_data, output_data)\n",
    "    performDataQualityChecks(df)\n",
    "    print(\"\\n\\n\")\n",
    "    df = process_immigration_data(spark, input_data, output_data)\n",
    "    performDataQualityChecks(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop rows with nulls in all columns of data frame:  ['City', 'State', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size', 'State Code', 'Race', 'Count']\n",
      "Rows before dropping nulls:  2891\n",
      "Rows after dropping nulls:  2875\n",
      "Rows after dropping duplicate rows:  2875\n",
      "+----------+--------------------+---------------+-----------------+----------------+------------------+------------+----------------------+------------------+---+\n",
      "|State_Code|               State|Male_Population|Female_Population|Total_Population|Number_Of_Veterans|Foreign_Born|Average_Household_Size|        Median_Age| Id|\n",
      "+----------+--------------------+---------------+-----------------+----------------+------------------+------------+----------------------+------------------+---+\n",
      "|        MT|             Montana|         438535|           467935|          906470|             69270|       29885|    2.2749999999999995|              35.5|  1|\n",
      "|        NC|      North Carolina|        7330525|          7970470|        15300995|            830730|     1896635|    2.4750000000000005| 33.78571428571429|  2|\n",
      "|        MD|            Maryland|        3139755|          3420890|         6560645|            320715|     1148970|                 2.655|36.370000000000005|  3|\n",
      "|        CO|            Colorado|        7273095|          7405250|        14678345|            939480|     1688155|    2.5599999999999987|35.818749999999994|  4|\n",
      "|        CT|         Connecticut|        2123435|          2231661|         4355096|            122546|     1114250|     2.666153846153846|  35.0025641025641|  5|\n",
      "|        IL|            Illinois|       10943864|         11570526|        22514390|            723049|     4632600|    2.7318681318681315| 35.70879120879121|  6|\n",
      "|        NJ|          New Jersey|        3423033|          3507991|         6931024|            146632|     2327750|     2.960877192982456| 35.25438596491229|  7|\n",
      "|        DE|            Delaware|         163400|           196385|          359785|             15315|       16680|                  2.45|              36.4|  8|\n",
      "|        DC|District of Columbia|        1598525|          1762615|         3361140|            129815|      475585|                  2.24|              33.8|  9|\n",
      "|        TN|           Tennessee|        5124189|          5565976|        10690165|            586202|      900149|    2.4629545454545454| 34.31136363636363| 10|\n",
      "|        LA|           Louisiana|        3134990|          3367985|         6502975|            348855|      417095|                 2.465| 34.62500000000001| 11|\n",
      "|        AR|            Arkansas|        1400724|          1482165|         2882889|            154390|      307753|     2.526896551724138| 32.73793103448276| 12|\n",
      "|        AK|              Alaska|         764725|           728750|         1493475|            137460|      166290|                  2.77|              32.2| 13|\n",
      "|        CA|          California|       61055672|         62388681|       123444353|           4617022|    37059662|    3.0953254437869817| 36.17396449704143| 14|\n",
      "|        NM|          New Mexico|        2045050|          2150160|         4195210|            302370|      445560|                 2.585|            37.775| 15|\n",
      "|        UT|                Utah|        2586752|          2532925|         5119677|            193165|      651811|              3.156875|30.862499999999983| 16|\n",
      "|        MI|            Michigan|        5217245|          5667993|        10885238|            490435|     1214547|    2.5055696202531648| 37.01139240506328| 17|\n",
      "|        NY|            New York|       23422799|         25579256|        49002055|           1019097|    17186873|    2.7703703703703697| 35.57037037037037| 18|\n",
      "|        NH|       New Hampshire|         488855|           502135|          990990|             55025|      135995|    2.4299999999999997|37.800000000000004| 19|\n",
      "|        WA|          Washington|        6228025|          6272510|        12500535|            765630|     2204810|     2.597647058823529| 35.28823529411764| 20|\n",
      "+----------+--------------------+---------------+-----------------+----------------+------------------+------------+----------------------+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- State_Code: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Male_Population: long (nullable = true)\n",
      " |-- Female_Population: long (nullable = true)\n",
      " |-- Total_Population: long (nullable = true)\n",
      " |-- Number_Of_Veterans: long (nullable = true)\n",
      " |-- Foreign_Born: long (nullable = true)\n",
      " |-- Average_Household_Size: double (nullable = true)\n",
      " |-- Median_Age: double (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "drop rows with nulls in all columns of data frame:  ['ident', 'type', 'name', 'iso_country', 'iso_region', 'coordinates']\n",
      "Rows before dropping nulls:  55075\n",
      "Rows after dropping nulls:  55075\n",
      "Rows after dropping duplicate rows:  55075\n",
      "+-----+-------------+--------------------+-----------+----------+----------+--------------------+---+\n",
      "|ident|         type|                name|iso_country|iso_region|state_code|         coordinates| Id|\n",
      "+-----+-------------+--------------------+-----------+----------+----------+--------------------+---+\n",
      "| 08CO|small_airport| Terra Firma Airport|         US|     US-CO|        CO|-104.041000366210...|  1|\n",
      "|  09N|small_airport|    Airhaven Airport|         US|     US-NY|        NY|-73.8759994506836...|  2|\n",
      "| 0FD3|small_airport|        Dugger Field|         US|     US-FL|        FL|-86.0938034057617...|  3|\n",
      "|  0G5|seaplane_base|Grand Marais/Cook...|         US|     US-MN|        MN|   -90.3835, 47.8263|  4|\n",
      "| 0NA5|small_airport|      Sorlie Airport|         US|     US-ND|        ND|-99.5628967285156...|  5|\n",
      "| 0TA3|small_airport|  Tate Ranch Airport|         US|     US-TX|        TX|-102.138000488281...|  6|\n",
      "| 0TE2|     heliport|Bell Helicopter H...|         US|     US-TX|        TX|-97.1669998168945...|  7|\n",
      "| 13WA|       closed|Fsa - Everett Hel...|         US|     US-WA|        WA|    -122.25, 47.9365|  8|\n",
      "| 14TN|     heliport|News Channel 3 He...|         US|     US-TN|        TN|-90.0714035034179...|  9|\n",
      "|  1E8|small_airport|      Moores Airport|         US|     US-NY|        NY|-75.0662994384765...| 10|\n",
      "| 1MT0|small_airport|Nine Quarter Circ...|         US|     US-MT|        MT|-111.296997070312...| 11|\n",
      "| 1ND4|small_airport|  Walkinshaw Airport|         US|     US-ND|        ND|-97.0169982910156...| 12|\n",
      "| 1XS6|     heliport|Hillcrest Baptist...|         US|     US-TX|        TX|-97.158528, 31.48...| 13|\n",
      "| 26AZ|small_airport|Flying Dare's Ran...|         US|     US-AZ|        AZ|-113.203002929687...| 14|\n",
      "| 2LA7|small_airport|    Costello Airport|         US|     US-LA|        LA|-91.4262008666992...| 15|\n",
      "| 2TE9|     heliport|Cuero Community H...|         US|     US-TX|        TX|-97.2833023071289...| 16|\n",
      "| 37OR|small_airport|Vey Sheep Ranch A...|         US|     US-OR|        OR|-118.399002075195...| 17|\n",
      "| 39ID|       closed|Albion Municipal ...|         US|     US-ID|        ID|  -113.5603, 42.4007| 18|\n",
      "| 3XA9|     heliport|Mfs Sabine Pass H...|         US|     US-TX|        TX|-93.868333, 29.72...| 19|\n",
      "| 40ME|small_airport|           Tib Field|         US|     US-ME|        ME|-70.955978, 44.10...| 20|\n",
      "+-----+-------------+--------------------+-----------+----------+----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "drop rows with nulls in all columns of data frame:  ['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'State', 'Country']\n",
      "Rows before dropping nulls:  149745\n",
      "Rows after dropping nulls:  141930\n",
      "Rows after dropping duplicate rows:  141930\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+-----------+---+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|  State|      Country|State_Codes| Id|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+-----------+---+\n",
      "|1844-04-01 00:00:00|             19.01|                         1.87|Alabama|United States|         AL|  1|\n",
      "|1901-09-01 00:00:00|             21.99|                         0.41|Alabama|United States|         AL|  2|\n",
      "|1902-06-01 00:00:00|             26.49|                         0.46|Alabama|United States|         AL|  3|\n",
      "|1915-11-01 00:00:00|             13.36|                         0.27|Alabama|United States|         AL|  4|\n",
      "|1923-03-01 00:00:00|             12.28|                         0.32|Alabama|United States|         AL|  5|\n",
      "|1923-07-01 00:00:00|             25.42|                         0.25|Alabama|United States|         AL|  6|\n",
      "|1924-10-01 00:00:00|             17.21|                         0.18|Alabama|United States|         AL|  7|\n",
      "|1944-04-01 00:00:00|              16.3|                         0.15|Alabama|United States|         AL|  8|\n",
      "|1945-11-01 00:00:00|             13.19|                         0.22|Alabama|United States|         AL|  9|\n",
      "|1994-04-01 00:00:00|             18.41|                         0.16|Alabama|United States|         AL| 10|\n",
      "|2005-08-01 00:00:00|             27.51|                         0.16|Alabama|United States|         AL| 11|\n",
      "|1851-12-01 00:00:00|            -19.71|                         4.65| Alaska|United States|         AK| 12|\n",
      "|1863-07-01 00:00:00|             11.32|                         3.89| Alaska|United States|         AK| 13|\n",
      "|1868-03-01 00:00:00|            -14.05|                         4.02| Alaska|United States|         AK| 14|\n",
      "|1891-01-01 00:00:00|            -18.31|                          3.9| Alaska|United States|         AK| 15|\n",
      "|1918-02-01 00:00:00|            -19.13|                         0.87| Alaska|United States|         AK| 16|\n",
      "|1925-11-01 00:00:00|            -11.46|                         0.77| Alaska|United States|         AK| 17|\n",
      "|1968-02-01 00:00:00|            -19.38|                         0.27| Alaska|United States|         AK| 18|\n",
      "|1975-08-01 00:00:00|              9.64|                         0.53| Alaska|United States|         AK| 19|\n",
      "|1975-12-01 00:00:00|            -22.12|                         0.22| Alaska|United States|         AK| 20|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- State_Codes: string (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "drop rows with nulls in all columns of data frame:  ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count', 'dtadfile', 'entdepa', 'entdepd', 'matflag', 'biryear', 'dtaddto', 'airline', 'admnum', 'fltno', 'visatype']\n",
      "Rows before dropping nulls:  3096313\n",
      "Rows after dropping nulls:  2769644\n",
      "Rows after dropping duplicate rows:  2769644\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Start_Time: string (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Week: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Weekday: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Start_Time: string (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Week: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Weekday: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Start_Time: string (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Week: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Weekday: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Start_Time: string (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Week: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Weekday: string (nullable = true)\n",
      "\n",
      "ArrivalDateTimeDimensionTable\n",
      "+----+--------------------+----+---+----+-----+----+-------+\n",
      "|  Id|          Start_Time|Hour|Day|Week|Month|Year|Weekday|\n",
      "+----+--------------------+----+---+----+-----+----+-------+\n",
      "| 432|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "| 630|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1014|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1037|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1116|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1227|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1335|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1340|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1652|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1763|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|2407|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|2480|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|2855|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|2939|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|3389|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|3603|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|4124|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|4131|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|4340|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|4669|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "+----+--------------------+----+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DepartureDateTimeDimensionTable\n",
      "+----+--------------------+----+---+----+-----+----+-------+\n",
      "|  Id|          Start_Time|Hour|Day|Week|Month|Year|Weekday|\n",
      "+----+--------------------+----+---+----+-----+----+-------+\n",
      "|  66|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "| 105|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "| 242|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "| 304|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "| 688|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "| 718|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1316|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1459|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1565|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1636|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1665|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1878|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|1909|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|2227|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|2405|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|2422|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|2615|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|3338|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|3342|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "|3343|1970-01-01 00:00:...|   0|  1|   1|    1|1970|      4|\n",
      "+----+--------------------+----+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+------+------+------+------+-------+-------+-------+------+-------+-----+--------+-------+-------+-------+-------+--------+-------+---------------+-----+--------+---+\n",
      "| cicid| i94yr|i94mon|i94cit|i94res|i94port|i94mode|i94addr|i94bir|i94visa|count|dtadfile|entdepa|entdepd|matflag|biryear| dtaddto|airline|         admnum|fltno|visatype| Id|\n",
      "+------+------+------+------+------+-------+-------+-------+------+-------+-----+--------+-------+-------+-------+-------+--------+-------+---------------+-----+--------+---+\n",
      "| 410.0|2016.0|   4.0| 103.0| 103.0|    MIA|    1.0|     FL|  35.0|    2.0|  1.0|20160401|      G|      O|      M| 1981.0|06292016|     TK|5.5457215033E10|00077|      WT|  1|\n",
      "| 411.0|2016.0|   4.0| 103.0| 103.0|    MIA|    1.0|     FL|  34.0|    2.0|  1.0|20160401|      G|      O|      M| 1982.0|06292016|     TK|5.5457375533E10|00077|      WT|  2|\n",
      "| 898.0|2016.0|   4.0| 104.0| 104.0|    NEW|    1.0|     NJ|  13.0|    2.0|  1.0|20160401|      G|      O|      M| 2003.0|06292016|     DL|5.5443159933E10|00021|      WT|  3|\n",
      "|1073.0|2016.0|   4.0| 104.0| 104.0|    NEW|    1.0|     NY|  62.0|    2.0|  1.0|20160401|      G|      O|      M| 1954.0|06292016|     UA|5.5438332733E10|02067|      WT|  4|\n",
      "|1165.0|2016.0|   4.0| 104.0| 104.0|    WAS|    1.0|     NY|  18.0|    2.0|  1.0|20160401|      G|      O|      M| 1998.0|06292016|     UA|5.5427397833E10|00947|      WT|  5|\n",
      "|1210.0|2016.0|   4.0| 104.0| 104.0|    HOU|    1.0|     TX|  15.0|    2.0|  1.0|20160401|      G|      O|      M| 2001.0|06292016|     UA|5.5441643733E10|00021|      WT|  6|\n",
      "|1249.0|2016.0|   4.0| 104.0| 104.0|    NYC|    1.0|     NY|  65.0|    2.0|  1.0|20160401|      G|      O|      M| 1951.0|06292016|     LH|5.5457152833E10|00404|      WT|  7|\n",
      "|1403.0|2016.0|   4.0| 104.0| 104.0|    NYC|    1.0|     NY|  42.0|    2.0|  1.0|20160401|      G|      O|      M| 1974.0|06292016|     AF|5.5437131933E10|00012|      WT|  8|\n",
      "|1449.0|2016.0|   4.0| 104.0| 104.0|    NYC|    1.0|     NY|  18.0|    2.0|  1.0|20160401|      G|      O|      M| 1998.0|06292016|     DL|5.5418481333E10|00049|      WT|  9|\n",
      "|1597.0|2016.0|   4.0| 104.0| 104.0|    LOS|    1.0|     CA|  36.0|    2.0|  1.0|20160401|      G|      O|      M| 1980.0|06292016|     NZ|5.5462204533E10|00001|      WT| 10|\n",
      "|1618.0|2016.0|   4.0| 104.0| 104.0|    MIA|    1.0|     FL|  48.0|    2.0|  1.0|20160401|      G|      I|      M| 1968.0|06292016|     UX|5.5460406033E10|00097|      WT| 11|\n",
      "|2025.0|2016.0|   4.0| 104.0| 104.0|    NYC|    1.0|     NY|  51.0|    2.0|  1.0|20160401|      O|      O|      M| 1965.0|06292016|     SN|5.5419979133E10|01401|      WT| 12|\n",
      "|2048.0|2016.0|   4.0| 104.0| 104.0|    MIA|    1.0|     FL|   3.0|    2.0|  1.0|20160401|      O|      O|      M| 2013.0|06292016|     UX|5.5456895133E10|00097|      WT| 13|\n",
      "|2651.0|2016.0|   4.0| 107.0| 107.0|    ORL|    1.0|     FL|  41.0|    2.0|  1.0|20160401|      G|      O|      M| 1975.0|09302016|     LH| 9.249973163E10|00464|      B2| 14|\n",
      "|2935.0|2016.0|   4.0| 108.0| 108.0|    DET|    1.0|     SC|  18.0|    2.0|  1.0|20160401|      G|      O|      M| 1998.0|06292016|     DL|5.5449595833E10|00139|      WT| 15|\n",
      "|3120.0|2016.0|   4.0| 108.0| 108.0|    NYC|    1.0|     NY|  38.0|    2.0|  1.0|20160401|      G|      O|      M| 1978.0|06292016|     BA|5.5459374833E10|00113|      WT| 16|\n",
      "|3554.0|2016.0|   4.0| 108.0| 108.0|    SAJ|    1.0|     VQ|  62.0|    2.0|  1.0|20160401|      G|      O|      M| 1954.0|06292016|     DY|5.5420924233E10|07125|      WT| 17|\n",
      "|3801.0|2016.0|   4.0| 109.0| 109.0|    LOS|    1.0|     CA|  36.0|    2.0|  1.0|20160401|      O|      O|      M| 1980.0|06292016|     AA|5.5450375933E10|00109|      WT| 18|\n",
      "|4040.0|2016.0|   4.0| 110.0| 110.0|    NYC|    1.0|     NY|  34.0|    2.0|  1.0|20160401|      G|      O|      M| 1982.0|06292016|     AY|5.5447662533E10|00005|      WT| 19|\n",
      "|4339.0|2016.0|   4.0| 110.0| 111.0|    BOS|    1.0|     MA|  17.0|    2.0|  1.0|20160401|      G|      O|      M| 1999.0|06292016|     BA|5.5460412433E10|00215|      WT| 20|\n",
      "+------+------+------+------+------+-------+-------+-------+------+-------+-----+--------+-------+-------+-------+-------+--------+-------+---------------+-----+--------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Querying on the resulting parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "result_data = \"/home/workspace/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_immigration = result_data + 'Immigration_Details/part-*.parquet'\n",
    "data_arrival =  result_data + 'ArrivalDateTime.parquet/year=1970/month=1/part-*.parquet'\n",
    "data_demographics = result_data + 'Demographics/part-*.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration = spark.read.parquet(data_immigration) \n",
    "df_arrival = spark.read.parquet(data_arrival) \n",
    "df_demographics = spark.read.parquet(data_demographics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration.createOrReplaceTempView(\"view_immigration\")\n",
    "df_arrival.createOrReplaceTempView(\"view_arrival\")\n",
    "df_demographics.createOrReplaceTempView(\"view_demographics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# query to find details of passenger with specific Id and associated demographics info\n",
    "query1 = \"select i.id, i.i94cit, i.i94port, i.i94mode, i.i94addr, i.airline, i.fltno, d.male_population, d.female_population, d.median_age from view_immigration as i inner join view_demographics as d on i.i94addr = d.state_code where i.id = 1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# query to find details of passenger with specific Id and associated arrival times info\n",
    "query2 = \"select i.id, i.i94cit, i.i94port, i.i94mode, i.i94addr, i.airline, i.fltno, a.hour, a.week, a.day from view_immigration as i inner join view_arrival as a on i.id = a.id where i.id = 1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+-------+-------+-------+-----+---------------+-----------------+-----------------+\n",
      "|  id|i94cit|i94port|i94mode|i94addr|airline|fltno|male_population|female_population|       median_age|\n",
      "+----+------+-------+-------+-------+-------+-----+---------------+-----------------+-----------------+\n",
      "|1000| 117.0|    CHI|    1.0|     IL|     TK|00005|       10943864|         11570526|35.70879120879121|\n",
      "+----+------+-------+-------+-------+-------+-----+---------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+-------+-------+-------+-----+----+----+---+\n",
      "|  id|i94cit|i94port|i94mode|i94addr|airline|fltno|hour|week|day|\n",
      "+----+------+-------+-------+-------+-------+-----+----+----+---+\n",
      "|1000| 117.0|    CHI|    1.0|     IL|     TK|00005|   0|   1|  1|\n",
      "+----+------+-------+-------+-------+-------+-----+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
